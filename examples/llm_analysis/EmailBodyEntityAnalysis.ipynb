{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de89e4f9",
   "metadata": {},
   "source": [
    "# Email body content analysis \n",
    "\n",
    "*For the context:* Current analysis in bigbang focus on headers. There are many analysis on the headers in the emails for the people and orgnization involved in the discussions. There are a few content analysis focusing on the keywords first occurence searching and/or most used words per user.\n",
    "\n",
    "This notebook analyze the email body contents with Huggingface Named Entity Recognition(NER) models that are able to systematically label the entities and their types(currently supports PER, ORG, LOC, and MISC) in the email bodies. This can potentially help the researchers understand more on the email conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aadfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from bigbang.archive import Archive\n",
    "from bigbang.archive import load as load_archive\n",
    "import pandas as pd\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5230a",
   "metadata": {},
   "source": [
    "First, use the script ```bin/collect_mail.py``` to collect web archives. Details can be seen in https://bigbang-py.readthedocs.io/en/latest/data-sources.html#id1 .\n",
    "\n",
    "<!-- Here, we use an example of the [scipy-dev](https://mail.python.org/pipermail/scipy-dev/) mailing list page.\n",
    "\n",
    "Scipy-dev mailing list contains 149,718 emails From June 2001 - September 2021. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d286f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mailing_list = \"3gv6\"\n",
    "archive_path = \"../../archives/{}/\".format(mailing_list)\n",
    "\n",
    "archive = Archive(archive_path,mbox=True)\n",
    "# archive data in pandas dataframe format\n",
    "archive_data = archive.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a767ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Date</th>\n",
       "      <th>In-Reply-To</th>\n",
       "      <th>References</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NALASEXMB04.na.qualcomm.com&gt;</th>\n",
       "      <td>\"Laganier, Julien\" &lt;julienl@qualcomm.com&gt;</td>\n",
       "      <td>Re: [3gv6] was draft report -  now  Ipv6 trans...</td>\n",
       "      <td>2009-11-20 19:01:16+00:00</td>\n",
       "      <td>&lt;C72C35BD.314D0%basavaraj.patil@nokia.com&gt;</td>\n",
       "      <td>&lt;4B0504D8.1070304@piuha.net&gt; &lt;C72C35BD.314D0%b...</td>\n",
       "      <td>Basavaraj Patil wrote:=20\\n&gt;=20\\n&gt; One other p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;C72C5914.314E1%basavaraj.patil@nokia.com&gt;</th>\n",
       "      <td>&lt;Basavaraj.Patil@nokia.com&gt;</td>\n",
       "      <td>[3gv6] Is this the Shanghai followup ML?</td>\n",
       "      <td>2009-11-20 20:32:40+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Hello,\\n\\nIs this ML setup for continuing disc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         From  \\\n",
       "Message-ID                                                                                      \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL...  \"Laganier, Julien\" <julienl@qualcomm.com>   \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>                        <Basavaraj.Patil@nokia.com>   \n",
       "\n",
       "                                                                                              Subject  \\\n",
       "Message-ID                                                                                              \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL...  Re: [3gv6] was draft report -  now  Ipv6 trans...   \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>                   [3gv6] Is this the Shanghai followup ML?   \n",
       "\n",
       "                                                                        Date  \\\n",
       "Message-ID                                                                     \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL... 2009-11-20 19:01:16+00:00   \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>         2009-11-20 20:32:40+00:00   \n",
       "\n",
       "                                                                                   In-Reply-To  \\\n",
       "Message-ID                                                                                       \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL...  <C72C35BD.314D0%basavaraj.patil@nokia.com>   \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>                                                None   \n",
       "\n",
       "                                                                                           References  \\\n",
       "Message-ID                                                                                              \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL...  <4B0504D8.1070304@piuha.net> <C72C35BD.314D0%b...   \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>                                                       None   \n",
       "\n",
       "                                                                                                 Body  \n",
       "Message-ID                                                                                             \n",
       "<BF345F63074F8040B58C00A186FCA57F1C65FB27A8@NAL...  Basavaraj Patil wrote:=20\\n>=20\\n> One other p...  \n",
       "<C72C5914.314E1%basavaraj.patil@nokia.com>          Hello,\\n\\nIs this ML setup for continuing disc...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data\n",
    "print(len(archive_data))\n",
    "archive_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3316be3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,\\n\\nIs this ML setup for continuing discussions pertaining to IPv6 transition i=\\nn\\n3GPP networks (followup to the meeting in Shanghai)?\\n\\n-Raj']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of one email body\n",
    "list(archive_data['Body'].iloc[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbc5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment line below to install transformers with pytorch \n",
    "# inside your current python environment\n",
    "\n",
    "# !pip install transformers[torch]\n",
    "# !pip install contractions\n",
    "# !pip install email_reply_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542b1fa",
   "metadata": {},
   "source": [
    "In pre-processing, we want to \n",
    "- remove the punctuations\n",
    "- remove links \n",
    "- expand contractions \n",
    "- remove digits\n",
    "- tokenize the words\n",
    "- [Optional] Lowercase the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7175c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for analyzing\n",
    "from bigbang.analysis.entity_recognition import EntityRecognizer, SpanVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac1bba",
   "metadata": {},
   "source": [
    "The list of models can be found in: https://huggingface.co/ . You can also train your own model and upload to huggingface.\n",
    "\n",
    "Examples for possible model names include:\n",
    "['dslim/bert-base-NER', 'dslim/bert-base-NER-cased', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbd42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the model and apply inference in the back-end of the bigbang package\n",
    "# you can pass the model name of your interest to the function\n",
    "model_name = \"EffyLi/bert-base-NER-finetuned-ner-cerec\"\n",
    "\n",
    "recognizer = EntityRecognizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd9a18c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text body before pre-processing--------------------:\n",
      "\n",
      "Raj, \n",
      "\n",
      "Excuse me for late reply, Inline.\n",
      "> Hui,\n",
      "> On 11/19/09 12:53 AM, \"Hui Deng\" <denghui@chinamobile.com> wrote:\n",
      "> > Hi, Sri,\n",
      "> >\n",
      "> > Just a short cut in here.\n",
      "> > DS-Lite =  Encapsulation +NAT44.\n",
      "> > PNAT (464) = Header change +NAT44,\n",
      "> > Principally, there is no big difference, but PNAT supports more other\n",
      "> > scenarios at the same time.\n",
      "> \n",
      "> The comparison sounds overly simplistic. PNAT (464) requires a shim in the\n",
      "> host and state. And the scenarios themselves need to be really reviewed in\n",
      "> terms of whether they can only be solved by one approach. I am yet to see\n",
      "a\n",
      "> scenario that can only be solved by one PNAT and not by DS-Lite. Maybe the\n",
      "> DS-Lite approach requires you to go through a GW and a NAT in order to\n",
      "> communicate with another host on the same network. But that in itself is\n",
      "not\n",
      "> an issue (IMO).\n",
      "Are u saying 4-6 or 4-4?\n",
      "even 4-4, they have overlapped address issue?\n",
      "\n",
      "-Hui\n",
      "> \n",
      "> Cheers,\n",
      "> -Raj\n",
      "> >\n",
      "> > -Hui\n",
      "> >\n",
      "Text body after pre-processing--------------------:\n",
      "\n",
      "Raj   Excuse me for late reply Inline a not Are you saying  or  even  they have overlapped address issue\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# hyperparams\n",
    "# taking one row as an example\n",
    "index = 6\n",
    "lowercase = False\n",
    "\n",
    "body = list(archive_data['Body'].iloc[[index]])[0]\n",
    "print(\"Text body before pre-processing--------------------:\\n\")\n",
    "print(body)\n",
    "body = recognizer.pre_processing(body, lowercase=lowercase)\n",
    "print(\"Text body after pre-processing--------------------:\\n\")\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68358157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'word': 'Raj'}\n",
      "{'entity': 'B-PER', 'word': 'me'}\n",
      "{'entity': 'B-PER', 'word': 'you'}\n",
      "{'entity': 'B-PER', 'word': 'they'}\n"
     ]
    }
   ],
   "source": [
    "tokens = recognizer.tokenizer.tokenize(body)\n",
    "labels = recognizer.recognize(body)\n",
    "entities = recognizer.get_entities(labels)\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31837549",
   "metadata": {},
   "source": [
    "*[Optional]* We can also visualize the results with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db8cf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment following line to install spacy package\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "432cf36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Raj\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " Excuse \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    me\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " for late reply Inline a not Are \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    you\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " saying or even \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    they\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " have overlapped address issue </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span, Doc\n",
    "\n",
    "# # defining a score threshold on the recognized entities. only entity has scored above the threshold will show\n",
    "# threashold = 0.0\n",
    "find_all_caps = True\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vocab = nlp.tokenizer.vocab\n",
    "\n",
    "visualizer = SpanVisualizer()\n",
    "merged_tokens = visualizer.merge_tokens(tokens)\n",
    "doc = Doc(vocab=vocab, words=merged_tokens)\n",
    "doc = visualizer.get_doc_for_visualization(tokens, labels, body, doc, find_all_caps)\n",
    "\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23c2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity type:  PER\n",
      "\t- Raj\n",
      "\t- me\n",
      "\t- you\n",
      "\t- they\n"
     ]
    }
   ],
   "source": [
    "visualizer.get_list_per_type()\n",
    "entity_type_list = visualizer.entity_type\n",
    "for typ, ent_list in entity_type_list.items():\n",
    "    print(\"entity type: \", typ)\n",
    "    for ent in ent_list:\n",
    "        print(\"\\t-\", ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78f5a1",
   "metadata": {},
   "source": [
    "## Processing the whole mailing list\n",
    "\n",
    "In the end, we show one example of how to pass a list of emails and return a list of entities with types. We save them in a csv file for futher processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d86fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'you', 'me', 'my', 'mine', 'myself', 'your', 'yours',\n",
    "              'yourself', 'we', 'us', 'our', 'ours', 'ourselves', 'yourselves',\n",
    "              'he', 'him', 'himself', 'his', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "              'they', 'them', 'their', 'theirs', 'themself', 'themselves', 'this', 'that', 'something',\n",
    "              'these', 'those', 'someone', 'somebody', 'who', 'whom', 'whose', 'which', 'what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "168ef5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 398 emails in total\n",
      "0 emails processed, 398 emails left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4022 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 emails processed, 198 emails left.\n",
      "Extracted entities saved!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# taking a list of indexes as examples\n",
    "num_data = len(archive_data)\n",
    "# num_data = 50\n",
    "print(\"Process {} emails in total\".format(num_data))\n",
    "indexes = list(range(0, num_data))\n",
    "lowercase = False\n",
    "find_all_caps = True\n",
    "\n",
    "model_name = \"EffyLi/bert-base-NER-finetuned-ner-cerec\"\n",
    "# model_name = \"dslim/bert-base-NER\"\n",
    "recognizer = EntityRecognizer(model_name)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vocab = nlp.tokenizer.vocab\n",
    "save_file_path = \"../../archives/\"\n",
    "save_file_name = archive_path.split(\"/\")[-2] + '-entities.csv'\n",
    "columns_names = ['email_id', 'entity', 'type']\n",
    "df = pd.DataFrame(columns=columns_names)\n",
    "\n",
    "email_entity_types = defaultdict(list)\n",
    "\n",
    "# print('Process emails with id: ', indexes)\n",
    "for index in indexes:\n",
    "    if index % 200 == 0:\n",
    "        print(\"{} emails processed, {} emails left.\".format(index, (num_data-index)))\n",
    "    body = list(archive_data['Body'].iloc[[index]])[0]\n",
    "    body = recognizer.pre_processing(body, lowercase=lowercase)\n",
    "#     show email bodies after pre-processing\n",
    "#     print(body)\n",
    "\n",
    "    visualizer = SpanVisualizer()\n",
    "    # get labels from recognizer first\n",
    "    tokens = recognizer.tokenizer.tokenize(body)  \n",
    "    labels = recognizer.recognize(body)\n",
    "    # merge tokens and spans in visualizer\n",
    "    merged_tokens = visualizer.merge_tokens(tokens)\n",
    "    doc = Doc(vocab=vocab, words=merged_tokens)\n",
    "    doc = visualizer.get_doc_for_visualization(tokens, labels, body, doc, find_all_caps)\n",
    "    visualizer.get_list_per_type()\n",
    "    entity_type = visualizer.entity_type\n",
    "    for k, v in entity_type.items():\n",
    "        email_entity_types[k].extend(v)\n",
    "        for v_i in v:\n",
    "            # remove pronouns\n",
    "            if v_i.lower() not in stop_words:\n",
    "                new_row = {\"email_id\": index, \"entity\": v_i, \"type\": k}\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "df.to_csv(save_file_name)\n",
    "print(\"Extracted entities saved!\")\n",
    "\n",
    "# for typ, ent_list in email_entity_types.items():\n",
    "#     print(\"entity type: \", typ)\n",
    "#     for ent in ent_list:\n",
    "#         print(\"\\t-\", ent)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3fafe",
   "metadata": {},
   "source": [
    "## Run the cell below only to display pre-processed mailing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "877472e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extracted_entities/3gv6-entities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sf/gz0qsktx5mn3h6dv89kznn4c0000gn/T/ipykernel_24759/3012814902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# load pre-processed csv file to dataframe and display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'extracted_entities/3gv6-entities.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# get top 10 frequent entities for each category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bigbang/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extracted_entities/3gv6-entities.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load pre-processed csv file to dataframe and display\n",
    "file_path = 'extracted_entities/3gv6-entities.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# get top 10 frequent entities for each category\n",
    "categories = list(set(recognizer.model.config.id2label.values()))\n",
    "categories = list(set([c.split('-')[-1] if '-' in c else c for c in categories ]))\n",
    "\n",
    "for c in categories:\n",
    "    if c != \"O\":\n",
    "        if c ==\"PER\":\n",
    "            print(\"Top 10 occurence (pronouns excluded) for type: \", c)\n",
    "        else:\n",
    "            print(\"Top 10 occurence for type: \", c)\n",
    "        df_c = df.loc[df['type'] == c]\n",
    "        display_df = df_c['entity'].value_counts().rename_axis('entity').reset_index(name='counts')\n",
    "        display(display_df.head(10))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c3ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigbang] *",
   "language": "python",
   "name": "conda-env-bigbang-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
