{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de89e4f9",
   "metadata": {},
   "source": [
    "# Email body content analysis \n",
    "\n",
    "*For the context:* Current analysis in bigbang focus on headers. There are many analysis on the headers in the emails for the people and orgnization involved in the discussions. There are a few content analysis focusing on the keywords first occurence searching and/or most used words per user.\n",
    "\n",
    "This notebook analyze the email body contents with Huggingface Named Entity Recognition(NER) models that are able to systematically label the entities and their types(currently supports PER, ORG, LOC, and MISC) in the email bodies. This can potentially help the researchers understand more on the email conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aadfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from bigbang.archive import Archive\n",
    "from bigbang.archive import load as load_archive\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5230a",
   "metadata": {},
   "source": [
    "First, use the script ```bin/collect_mail.py``` to collect web archives. Details can be seen in https://bigbang-py.readthedocs.io/en/latest/data-sources.html#id1 .\n",
    "\n",
    "Here, we use an example of the [scipy-dev](https://mail.python.org/pipermail/scipy-dev/) mailing list page.\n",
    "\n",
    "Scipy-dev mailing list contains 149,718 emails From June 2001 - September 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d286f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = \"../../archives/scipy-dev/\"\n",
    "\n",
    "archive = Archive(archive_path,mbox=True)\n",
    "# archive data in pandas dataframe format\n",
    "archive_data = archive.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a767ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149718\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Date</th>\n",
       "      <th>In-Reply-To</th>\n",
       "      <th>References</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net&gt;</th>\n",
       "      <td>travis at vaught.net (Travis N. Vaught)</td>\n",
       "      <td>[SciPy-dev] SciPy Developer mailing list now o...</td>\n",
       "      <td>2001-06-11 02:10:51+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The link:\\n\\nhttp://scipy.net/mailman/listinfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;Pine.LNX.4.33.0107231957590.15960-100000@oliphant.ee.byu.edu&gt;</th>\n",
       "      <td>oliphant at ee.byu.edu (Travis Oliphant)</td>\n",
       "      <td>[SciPy-dev] RPMs and source distribution</td>\n",
       "      <td>2001-07-24 02:01:00+00:00</td>\n",
       "      <td>&lt;02f001c111bf$2e78a9d0$777ba8c0@190xb01&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>I've been playing for hours and finally have i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        From  \\\n",
       "Message-ID                                                                                     \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>     travis at vaught.net (Travis N. Vaught)   \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph...  oliphant at ee.byu.edu (Travis Oliphant)   \n",
       "\n",
       "                                                                                              Subject  \\\n",
       "Message-ID                                                                                              \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>    [SciPy-dev] SciPy Developer mailing list now o...   \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph...           [SciPy-dev] RPMs and source distribution   \n",
       "\n",
       "                                                                        Date  \\\n",
       "Message-ID                                                                     \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>   2001-06-11 02:10:51+00:00   \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph... 2001-07-24 02:01:00+00:00   \n",
       "\n",
       "                                                                                 In-Reply-To  \\\n",
       "Message-ID                                                                                     \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>                                        None   \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph...  <02f001c111bf$2e78a9d0$777ba8c0@190xb01>   \n",
       "\n",
       "                                                   References  \\\n",
       "Message-ID                                                      \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>         None   \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph...       None   \n",
       "\n",
       "                                                                                                 Body  \n",
       "Message-ID                                                                                             \n",
       "<NEBBIECAMLMAAKHEGPCGKEBHCLAA.travis@vaught.net>    The link:\\n\\nhttp://scipy.net/mailman/listinfo...  \n",
       "<Pine.LNX.4.33.0107231957590.15960-100000@oliph...  I've been playing for hours and finally have i...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data which contains 7 columns and 149718 entries\n",
    "print(archive_data.size)\n",
    "archive_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3316be3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've been playing for hours and finally have it so that\\n\\npython setup.py sdist\\npython setup.by bdist_rpm\\n\\nwork as expected.\\n\\nI have distributions and RPM's that I need to put somewhere.\\n\\nThanks,\\n\\n-Travis\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of one email body\n",
    "list(archive_data['Body'].iloc[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbc5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment line below to install transformers with pytorch \n",
    "# inside your current python environment\n",
    "\n",
    "# !pip install transformers[torch]\n",
    "# !pip install contractions\n",
    "# !pip install email_reply_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542b1fa",
   "metadata": {},
   "source": [
    "In pre-processing, we want to \n",
    "- remove the punctuations\n",
    "- remove links \n",
    "- expand contractions \n",
    "- remove digits\n",
    "- tokenize the words\n",
    "- [Optional] Lowercase the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7175c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for analyzing\n",
    "from bigbang.analysis.entity_recognition import EntityRecognizer, SpanVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac1bba",
   "metadata": {},
   "source": [
    "The list of models can be found in: https://huggingface.co/ . You can also train your own model and upload to huggingface.\n",
    "\n",
    "Examples for possible model names include:\n",
    "['dslim/bert-base-NER', 'dslim/bert-base-NER-cased', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbd42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the model and apply inference in the back-end of the bigbang package\n",
    "# you can pass the model name of your interest to the function\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "recognizer = EntityRecognizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9a18c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text body before pre-processing--------------------:\n",
      "\n",
      "I'd say the latter of the two.  I started linalg months ago, and Travis O.\n",
      "put\n",
      "a lot of effort into over the last several weeks.\n",
      "\n",
      "I'm not really familiar with 3.0 -- we are really focusing on ATLAS cause\n",
      "it is so dang fast on most platforms.  It doesn't provide a full LAPACK\n",
      "though,\n",
      "so you have to merge it with another LAPACK to get everything.\n",
      "\n",
      "If you can figure out how to write a generic interface (not to hard, but\n",
      "only\n",
      "partially documented in /linalg/docs/more_notes), then have at it.\n",
      "The actual f2py interfaces are generated from a python script.\n",
      "\n",
      "The more interfaces the merrier, but the compatibility issue has to be\n",
      "addressed.\n",
      "On Unix, we could use 'nm' to check if the function is there.  On windows it\n",
      "ain't so easy.  Maybe it should just be an optional function for now (i.e.\n",
      "defaults\n",
      "to being commented out) for the widest compatibility.\n",
      "\n",
      "eric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----- Original Message -----\n",
      "From: \"Robert Kern\" <kern at caltech.edu>\n",
      "To: <scipy-dev at scipy.org>\n",
      "Sent: Thursday, September 06, 2001 4:36 PM\n",
      "Subject: [SciPy-dev] scipy.linalg and LAPACK 3.0\n",
      "\n",
      "\n",
      "> Hi,\n",
      ">\n",
      "> Some of the routines wrapped in generic_lapack.pyf seem to be deprecated\n",
      "in\n",
      "> LAPACK 3.0, notably _GESVD, which is rather slower than the new _GESDD. Is\n",
      "there\n",
      "> a reason for this (e.g. compatibility for people who haven't upgraded\n",
      "their\n",
      "> LAPACK, yet), or has simply no-one done the work, yet?\n",
      ">\n",
      "> If the latter, I'll see what I can come up with and contribute.\n",
      ">\n",
      "> --\n",
      "> Robert Kern\n",
      "> kern at caltech.edu\n",
      ">\n",
      "> \"In the fields of hell where the grass grows high\n",
      ">  Are the graves of dreams allowed to die.\"\n",
      ">   -- Richard Harter\n",
      "> _______________________________________________\n",
      "> Scipy-dev mailing list\n",
      "> Scipy-dev at scipy.net\n",
      "> http://www.scipy.net/mailman/listinfo/scipy-dev\n",
      "Text body after pre-processing--------------------:\n",
      "\n",
      "I would say the latter of the two  I started linalg months ago and Travis O put a lot of effort into over the last several weeks  I am not really familiar with   we are really focusing on ATLAS because it is so dang fast on most platforms  It does not provide a full LAPACK though so you have to merge it with another LAPACK to get everything  If you can figure out how to write a generic interface not to hard but only partially documented in linalgdocsmore_notes then have at it The actual fpy interfaces are generated from a python script  The more interfaces the merrier but the compatibility issue has to be addressed On Unix we could use nm to check if the function is there  On windows it are not so easy  Maybe it should just be an optional function for now ie defaults to being commented out for the widest compatibility  eric\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# hyperparams\n",
    "# taking one row as an example\n",
    "index = 20\n",
    "lowercase = False\n",
    "\n",
    "body = list(archive_data['Body'].iloc[[index]])[0]\n",
    "print(\"Text body before pre-processing--------------------:\\n\")\n",
    "print(body)\n",
    "body = recognizer.pre_processing(body, lowercase=lowercase)\n",
    "print(\"Text body after pre-processing--------------------:\\n\")\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68358157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'word': 'Travis'}\n",
      "{'entity': 'I-PER', 'word': 'O'}\n",
      "{'entity': 'B-ORG', 'word': 'AT'}\n",
      "{'entity': 'B-MISC', 'word': 'Unix'}\n"
     ]
    }
   ],
   "source": [
    "tokens = recognizer.tokenizer.tokenize(body)\n",
    "labels = recognizer.recognize(body)\n",
    "entities = recognizer.get_entities(labels)\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31837549",
   "metadata": {},
   "source": [
    "*[Optional]* We can also visualize the results with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8cf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment following line to install spacy package\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432cf36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I would say the latter of the two I started linalg months ago and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Travis O\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " put a lot of effort into over the last several weeks I am not really familiar with we are really focusing on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ATLAS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " because it is so dang fast on most platforms It does not provide a full \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    LAPACK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ALLCAPS</span>\n",
       "</mark>\n",
       " though so you have to merge it with another \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    LAPACK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ALLCAPS</span>\n",
       "</mark>\n",
       " to get everything If you can figure out how to write a generic interface not to hard but only partially documented in linalgdocsmore _ notes then have at it The actual fpy interfaces are generated from a python script The more interfaces the merrier but the compatibility issue has to be addressed On \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Unix\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " we could use nm to check if the function is there On windows it are not so easy Maybe it should just be an optional function for now ie defaults to being commented out for the widest compatibility eric </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span, Doc\n",
    "\n",
    "# # defining a score threshold on the recognized entities. only entity has scored above the threshold will show\n",
    "# threashold = 0.0\n",
    "find_all_caps = True\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vocab = nlp.tokenizer.vocab\n",
    "\n",
    "visualizer = SpanVisualizer()\n",
    "merged_tokens = visualizer.merge_tokens(tokens)\n",
    "doc = Doc(vocab=vocab, words=merged_tokens)\n",
    "doc = visualizer.get_doc_for_visualization(tokens, labels, body, doc, find_all_caps)\n",
    "\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23c2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity type:  PER\n",
      "\t- Travis O\n",
      "entity type:  ORG\n",
      "\t- ATLAS\n",
      "entity type:  MISC\n",
      "\t- Unix\n",
      "entity type:  ALLCAPS\n",
      "\t- LAPACK\n"
     ]
    }
   ],
   "source": [
    "visualizer.get_list_per_type()\n",
    "entity_type_list = visualizer.entity_type\n",
    "for typ, ent_list in entity_type_list.items():\n",
    "    print(\"entity type: \", typ)\n",
    "    for ent in ent_list:\n",
    "        print(\"\\t-\", ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78f5a1",
   "metadata": {},
   "source": [
    "In the end, we show one example of how to pass a list of emails and return a list of entities with types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168ef5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process emails with id:  [40, 50, 60]\n",
      "entity type:  ORG\n",
      "\t- SHA\n",
      "\t- University of North Carolina\n",
      "\t- Department of Chemistry\n",
      "\t- Venable Hall\n",
      "\t- Kenan\n",
      "\t- Chapel Hill NC\n",
      "\t- Mailcrypt\n",
      "\t- Matlabs\n",
      "\t- C\n",
      "\t- Emacs\n",
      "\t- Freiheit\n",
      "entity type:  LOC\n",
      "\t- USA\n",
      "entity type:  ALLCAPS\n",
      "\t- BEGIN\n",
      "\t- PGP\n",
      "\t- SIGNED\n",
      "\t- MESSAGE\n",
      "\t- BCCDE\n",
      "\t- SIGNATURE\n",
      "\t- END\n",
      "\t- BCCDE\n",
      "entity type:  MISC\n",
      "\t- CVS\n",
      "entity type:  PER\n",
      "\t- Eric\n",
      "\t- Einigkeit\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# taking a list of indexes as examples\n",
    "indexes = [40, 50, 60]\n",
    "lowercase = False\n",
    "find_all_caps = True\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "recognizer = EntityRecognizer(model_name)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vocab = nlp.tokenizer.vocab\n",
    "\n",
    "email_entity_types = defaultdict(list)\n",
    "\n",
    "print('Process emails with id: ', indexes)\n",
    "for index in indexes:\n",
    "    body = list(archive_data['Body'].iloc[[index]])[0]\n",
    "    body = recognizer.pre_processing(body, lowercase=lowercase)\n",
    "#     show email bodies after pre-processing\n",
    "#     print(body)\n",
    "\n",
    "    visualizer = SpanVisualizer()\n",
    "    # get labels from recognizer first\n",
    "    tokens = recognizer.tokenizer.tokenize(body)  \n",
    "    labels = recognizer.recognize(body)\n",
    "    # merge tokens and spans in visualizer\n",
    "    merged_tokens = visualizer.merge_tokens(tokens)\n",
    "    doc = Doc(vocab=vocab, words=merged_tokens)\n",
    "    doc = visualizer.get_doc_for_visualization(tokens, labels, body, doc, find_all_caps)\n",
    "    visualizer.get_list_per_type()\n",
    "    entity_type = visualizer.entity_type\n",
    "    for k, v in entity_type.items():\n",
    "        email_entity_types[k].extend(v)\n",
    "\n",
    "for typ, ent_list in email_entity_types.items():\n",
    "    print(\"entity type: \", typ)\n",
    "    for ent in ent_list:\n",
    "        print(\"\\t-\", ent)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigbang] *",
   "language": "python",
   "name": "conda-env-bigbang-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
