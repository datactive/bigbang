{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This notebook tries to detect \"special words\" in a corpus of mailing lists.</b>\n",
    "(for now it works with two mailing lists only)\n",
    "\n",
    "-it computes and exports in .csv files the word counts (words and their occurrences)\n",
    "-it computes and exports in .csv files the  list of common words that are introduced by different people in different lists\n",
    "it computes and print the 'influential words' (see definition in the box)\n",
    "\n",
    "Further extensions:\n",
    "-from two lists to n lists !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigbang.archive import Archive\n",
    "from bigbang.archive import load as load_archive\n",
    "import bigbang.parse as parse\n",
    "import bigbang.graph as graph\n",
    "import bigbang.mailman as mailman\n",
    "import bigbang.process as process\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import pytz\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from itertools import repeat\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "st = LancasterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert TWO names of mailing lists (no more, no less)\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "archives_names = [\"ietf-privacy\", \"architecture-discuss\"]\n",
    "\n",
    "\n",
    "archives_paths = list()\n",
    "for archive_name in archives_names:\n",
    "    archives_paths.append(\"../../archives/\" + archive_name + \".csv\")\n",
    "\n",
    "\n",
    "archives_list = [load_archive(archive_path).data for archive_path in archives_paths]\n",
    "\n",
    "archives = Archive(pd.concat(archives_list))\n",
    "\n",
    "archives_data = archives.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to stem or not to stem?\n",
    "# if stem is set to True, then words are converted into their stem(no plurals, no suffixes, etc.)\n",
    "# if stem is set to False, then words are processed for their literal spelling\n",
    "\n",
    "stem = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall compute the word counts on the lists. \n",
    "Data will be also exported to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word count on the first list\n",
    "wordcount1 = {}\n",
    "for row in archives_list[0].iterrows():\n",
    "    if row[1][\"Body\"] is not None:\n",
    "        w = row[1][\"Body\"].replace(\"'\", \"\")\n",
    "        k = re.sub(r\"[^\\w]\", \" \", w)\n",
    "        t = nltk.tokenize.word_tokenize(k)\n",
    "        for g in t:\n",
    "            try:\n",
    "                if stem:\n",
    "                    word = st.stem(g)\n",
    "                else:\n",
    "                    word = g\n",
    "            except:\n",
    "                print(g)\n",
    "                pass\n",
    "            if word in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            if word not in wordcount1:\n",
    "                wordcount1[word] = [1]\n",
    "                wordcount1[word].append(row[0])\n",
    "                wordcount1[word].append(row[1][\"Date\"])\n",
    "                wordcount1[word].append(row[1][\"From\"])\n",
    "                wordcount1[word].append(row[1][\"In-Reply-To\"])\n",
    "            else:\n",
    "                wordcount1[word][0] += 1\n",
    "wd = wordcount  # In case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word count on the second list\n",
    "wordcount2 = {}\n",
    "for row in archives_list[1].iterrows():\n",
    "    if row[1][\"Body\"] is not None:\n",
    "        w = row[1][\"Body\"].replace(\"'\", \"\")\n",
    "        k = re.sub(r\"[^\\w]\", \" \", w)\n",
    "        t = nltk.tokenize.word_tokenize(k)\n",
    "        for g in t:\n",
    "            try:\n",
    "                if stem:\n",
    "                    word = st.stem(g)\n",
    "                else:\n",
    "                    word = g\n",
    "            except:\n",
    "                pass\n",
    "            if word in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            if word not in wordcount2:\n",
    "                wordcount2[word] = [1]\n",
    "                wordcount2[word].append(row[0])\n",
    "                wordcount2[word].append(row[1][\"Date\"])\n",
    "                wordcount2[word].append(row[1][\"From\"])\n",
    "                wordcount2[word].append(row[1][\"In-Reply-To\"])\n",
    "            else:\n",
    "                wordcount2[word][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exported!\n",
      "Check /home/berra/bigbang/examples/word_analysis/wordcount_info_ietf-privacy.csv and /home/berra/bigbang/examples/word_analysiswordcount_info_architecture-discuss.csv\n"
     ]
    }
   ],
   "source": [
    "# Create and export a wordcount information dataframe per mailing list\n",
    "\n",
    "# set the variable 'path' as a valid directory path where to store the files\n",
    "\n",
    "\n",
    "asd = pd.DataFrame(wordcount)\n",
    "new_dataframe = asd.transpose()\n",
    "new_dataframe.columns = [\"Wordcount\", \"Message-ID\", \"Date\", \"From\", \"In-Reply-To\"]\n",
    "new_dataframe.to_csv(cwd + \"/wordcount_info_\" + archives_names[0] + \".csv\")\n",
    "\n",
    "asd1 = pd.DataFrame(wordcount1)\n",
    "new_dataframe1 = asd1.transpose()\n",
    "new_dataframe1.columns = [\"Wordcount\", \"Message-ID\", \"Date\", \"From\", \"In-Reply-To\"]\n",
    "new_dataframe1.to_csv(cwd + \"/wordcount_info_\" + archives_names[1] + \".csv\")\n",
    "\n",
    "print(\"File exported!\")\n",
    "print(\n",
    "    \"Check \"\n",
    "    + cwd\n",
    "    + \"/wordcount_info_\"\n",
    "    + archives_names[0]\n",
    "    + \".csv and \"\n",
    "    + cwd\n",
    "    + \"wordcount_info_\"\n",
    "    + archives_names[1]\n",
    "    + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some useful descriptive data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in mailinglist ietf-privacy\n",
      "11982\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in mailinglist \" + archives_names[0])\n",
    "print(len(wordcount1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in mailinglist architecture-discuss\n",
      "14581\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in mailinglist \" + archives_names[1])\n",
    "print(len(wordcount2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of same unique words in two mailing lists\n",
      "5906\n"
     ]
    }
   ],
   "source": [
    "samewordcount = 0\n",
    "for word in wordcount1:\n",
    "    if word in wordcount2:\n",
    "        samewordcount += 1\n",
    "print(\"Number of same unique words in two mailing lists\")\n",
    "print(samewordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of same words that are introduced by same people\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "samewords = {}\n",
    "for word in wordcount1:\n",
    "    if word in wordcount2:\n",
    "        if wordcount1[word][3] == wordcount2[word][3]:\n",
    "            samewords[word] = [\n",
    "                wordcount1[word][0],\n",
    "                wordcount1[word][3],\n",
    "                wordcount1[word][2],\n",
    "                wordcount2[word][0],\n",
    "                wordcount2[word][3],\n",
    "                wordcount2[word][2],\n",
    "            ]\n",
    "print(\"Total number of same words that are introduced by same people\")\n",
    "print(len(list(samewords.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exported!\n",
      "Check /home/berra/bigbang/examples/word_analysis/samewords_sameauthor.csv\n"
     ]
    }
   ],
   "source": [
    "# build dataframe of information of those words introduced by same people\n",
    "# and export to file\n",
    "df1 = pd.DataFrame(samewords)\n",
    "samewords_sameauthor_dataframe = df1.transpose()\n",
    "samewords_sameauthor_dataframe.columns = [\n",
    "    \"Wordcount1\",\n",
    "    \"From1\",\n",
    "    \"Date1\",\n",
    "    \"Wordcount2\",\n",
    "    \"From2\",\n",
    "    \"Date2\",\n",
    "]\n",
    "samewords_sameauthor_dataframe.to_csv(cwd + \"/samewords_sameauthor.csv\")\n",
    "print(\"File exported!\")\n",
    "print(\"Check \" + cwd + \"/samewords_sameauthor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 100-500 appearance words, the number of common words between two mailing-list\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "samewordcount = 0\n",
    "for word in wordcount1:\n",
    "    if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "        if word in wordcount2:\n",
    "            if wordcount2[word][0] >= 100 and wordcount2[word][0] <= 500:\n",
    "                samewordcount += 1\n",
    "print(\n",
    "    \"Among 100-500 appearance words, the number of common words between two mailing-list\"\n",
    ")\n",
    "print(samewordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among 100-500 appearance words, the number of common words between two mailing-list that are first introduced by same people\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "same_person_count = 0\n",
    "for word in wordcount1:\n",
    "    if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "        if word in wordcount2:\n",
    "            if wordcount2[word][0] >= 100 and wordcount2[word][0] <= 500:\n",
    "                if wordcount1[word][3] == wordcount2[word][3]:\n",
    "                    # print word\n",
    "                    same_person_count += 1\n",
    "print(\n",
    "    \"Among 100-500 appearance words, the number of common words between two mailing-list that are first introduced by same people\"\n",
    ")\n",
    "print(same_person_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the list of common words that are introduced by different people in different lists.\n",
    "The results are exported in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words introduced by different people in different lists\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "# compute common word list(introduced by different people in different lists)\n",
    "# and print the number\n",
    "commonwords = {}\n",
    "for word in wordcount1:\n",
    "    if wordcount1[word][0] >= 100 and wordcount1[word][0] <= 500:\n",
    "        if word in wordcount2:\n",
    "            if wordcount2[word][0] >= 100 and wordcount2[word][0] <= 500:\n",
    "                if wordcount1[word][3] != wordcount2[word][3]:\n",
    "                    commonwords[word] = [\n",
    "                        wordcount1[word][0],\n",
    "                        wordcount1[word][3],\n",
    "                        wordcount1[word][2],\n",
    "                        wordcount2[word][0],\n",
    "                        wordcount2[word][3],\n",
    "                        wordcount2[word][2],\n",
    "                    ]\n",
    "print(\"Number of common words introduced by different people in different lists\")\n",
    "print(len(commonwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exported!\n",
      "Check /home/berra/bigbang/examples/word_analysis/commonwords_differentauthor.csv\n"
     ]
    }
   ],
   "source": [
    "# build dataframe of information of those words introduced by different people\n",
    "# and export to file\n",
    "df1 = pd.DataFrame(commonwords)\n",
    "commonword_differentauthor_dataframe = df1.transpose()\n",
    "commonword_differentauthor_dataframe.columns = [\n",
    "    \"Wordcount1\",\n",
    "    \"From1\",\n",
    "    \"Date1\",\n",
    "    \"Wordcount2\",\n",
    "    \"From2\",\n",
    "    \"Date2\",\n",
    "]\n",
    "commonword_differentauthor_dataframe.to_csv(cwd + \"/commonwords_differentauthor.csv\")\n",
    "print(\"File exported!\")\n",
    "print(\"Check \" + cwd + \"/commonwords_differentauthor.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify \"influential words\" (see definition below) and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 'influential words', the list of words that have potential of idea flows.\n",
    "\n",
    "# Definition: A is introduced by p in list1 first, then q saw it and then\n",
    "# introduced the word A to list 2, or vice versa. We defined q saw as q said something in list1 before p poped out the word.\n",
    "# Total list of such word A.\n",
    "\n",
    "\n",
    "# Build a dictionary with senders and date of first participation for each mailing list\n",
    "first_participation1 = {}\n",
    "for row in archives_list[0].iterrows():\n",
    "    if row[1][\"From\"] not in first_participation1:\n",
    "        first_participation1[row[1][\"From\"]] = row[1][\"Date\"]\n",
    "first_participation2 = {}\n",
    "for row in archives_list[1].iterrows():\n",
    "    if row[1][\"From\"] not in first_participation2:\n",
    "        first_participation2[row[1][\"From\"]] = row[1][\"Date\"]\n",
    "\n",
    "time_influence = 0\n",
    "influence_list = {}\n",
    "for word in commonwords:\n",
    "    if commonwords[word][2] > commonwords[word][5]:  # Author2 comes first\n",
    "        if commonwords[word][1] in first_participation2:  # Check if author1 in list2\n",
    "            if (\n",
    "                first_participation2[commonwords[word][1]] < commonwords[word][5]\n",
    "            ):  # Check if author1\\\n",
    "                # in list2 and exists before the word first introduced in list2\n",
    "                influence_list[word] = commonwords[word]\n",
    "                time_influence += 1\n",
    "    else:  # Author1 comes first\n",
    "        if commonwords[word][4] in first_participation1:\n",
    "            if first_participation1[commonwords[word][4]] < commonwords[word][2]:\n",
    "                influence_list[word] = commonwords[word]\n",
    "                time_influence += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No influential words detected\n"
     ]
    }
   ],
   "source": [
    "# print the list of influential words (exclude numbers)\n",
    "if len(list(influence_list.keys())) == 0:\n",
    "    print(\"No influential words detected\")\n",
    "for word, info in influence_list.items():\n",
    "    if not word.isdigit():\n",
    "        print('\"' + word + '\"')\n",
    "        print(info)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
